{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK 1 - Naive Approaches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Obtaining dependency information for pandas from https://files.pythonhosted.org/packages/c3/6c/ea362eef61f05553aaf1a24b3e96b2d0603f5dc71a3bd35688a24ed88843/pandas-2.0.3-cp38-cp38-win_amd64.whl.metadata\n",
      "  Downloading pandas-2.0.3-cp38-cp38-win_amd64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mally\\leiden university [2022 -2024]\\3rd sem\\aidm\\assignment 1\\.venv\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Obtaining dependency information for pytz>=2020.1 from https://files.pythonhosted.org/packages/32/4d/aaf7eff5deb402fd9a24a1449a8119f00d74ae9c2efa79f8ef9994261fc2/pytz-2023.3.post1-py2.py3-none-any.whl.metadata\n",
      "  Downloading pytz-2023.3.post1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.1 (from pandas)\n",
      "  Using cached tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\mally\\leiden university [2022 -2024]\\3rd sem\\aidm\\assignment 1\\.venv\\lib\\site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mally\\leiden university [2022 -2024]\\3rd sem\\aidm\\assignment 1\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.0.3-cp38-cp38-win_amd64.whl (10.8 MB)\n",
      "   ---------------------------------------- 0.0/10.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/10.8 MB 5.9 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.8/10.8 MB 8.0 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.2/10.8 MB 8.5 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.7/10.8 MB 9.0 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.3/10.8 MB 9.5 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.7/10.8 MB 9.4 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 3.1/10.8 MB 9.8 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 3.8/10.8 MB 10.2 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.5/10.8 MB 10.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 5.3/10.8 MB 10.5 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.7/10.8 MB 10.5 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.2/10.8 MB 10.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.2/10.8 MB 9.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 6.6/10.8 MB 9.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 6.8/10.8 MB 9.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.3/10.8 MB 9.3 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.8/10.8 MB 9.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.2/10.8 MB 9.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.8/10.8 MB 9.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.0/10.8 MB 9.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.8/10.8 MB 9.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.5/10.8 MB 9.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.8/10.8 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.8/10.8 MB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.8/10.8 MB 8.8 MB/s eta 0:00:00\n",
      "Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.0.3 pytz-2023.3.post1 tzdata-2023.3\n"
     ]
    }
   ],
   "source": [
    "#Install necessary libraries\n",
    "!pip install scikit-learn\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000209, 4)    UserID  MovieID  Rating  Timestamp\n",
      "0       1     1193       5  978300760\n",
      "1       1      661       3  978302109\n",
      "(6040, 5)    UserID Gender  Age  Occupation Zip-code\n",
      "0       1      F    1          10    48067\n",
      "1       2      M   56          16    70072\n",
      "(3883, 3)    MovieID             Title                        Genres\n",
      "0        1  Toy Story (1995)   Animation|Children's|Comedy\n",
      "1        2    Jumanji (1995)  Adventure|Children's|Fantasy\n"
     ]
    }
   ],
   "source": [
    "#Import necessary libraries\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the MovieLens 1M dataset using pandas library, We use the seperator as :: to separate the columns whose names were mentioned in the README file provided.\n",
    "ratings = pd.read_csv('ml-1m/ratings.dat', sep='::', engine='python',\n",
    "                      names=['UserID', 'MovieID', 'Rating', 'Timestamp'])\n",
    "print(ratings.shape,ratings.head(2))\n",
    "\n",
    "\n",
    "users = pd.read_csv('ml-1m/users.dat', sep='::', engine='python',\n",
    "                    names=['UserID', 'Gender','Age','Occupation','Zip-code'])\n",
    "print(users.shape,users.head(2))\n",
    "\n",
    "\n",
    "movies = pd.read_csv('ml-1m/movies.dat', sep='::', engine='python', encoding='latin-1',\n",
    "                     names=['MovieID','Title','Genres'])\n",
    "print(movies.shape,movies.head(2))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Custom K-fold cross-validation script for a dataset.\n",
    "\n",
    "This script divides a given dataset into 'n_splits' random folds for cross-validation. \n",
    "The option to shuffle the data is available, similar to the shuffle parameter in Scikit-Learn's KFold.\n",
    "Yielding can be more memory-efficient than returning when dealing with large datasets hence we use yielding instead of returning the indices.\n",
    "\n",
    "Parameters:\n",
    "- n_splits: Number of cross-validation splits (folds).\n",
    "- seed: Random seed for reproducibility.\n",
    "- data: The dataset to be split (Pandas DataFrame).\n",
    "- shuffle: Determines whether to shuffle the data before splitting (True for random shuffling, False for ordered splitting).\n",
    "\n",
    "Yields:\n",
    "- train_indices: Indices for the training set of the current fold.\n",
    "- test_indices: Indices for the testing set of the current fold.\n",
    "\"\"\"\n",
    "\n",
    "def K_fold(n_splits, seed,data,shuffle):\n",
    "    subset_size = len(data) // n_splits  # Calculate the size of each fold\n",
    "    if shuffle:\n",
    "        indices = data.index.to_numpy()  # Get the DataFrame's index as a NumPy array\n",
    "        np.random.seed(seed)  # Set the random seed for reproducibility\n",
    "        np.random.shuffle(indices)  # Shuffle the indices randomly\n",
    "    for fold in range(n_splits):\n",
    "        start = fold * subset_size  # Calculate the start index of the current fold\n",
    "        end = (fold + 1) * subset_size if fold < n_splits - 1 else len(data)    # Calculate the end index\n",
    "        test_indices = indices[start:end]                                       # Select indices for the testing set\n",
    "        train_indices = np.concatenate([indices[:start], indices[end:]])        # Select indices for the training set\n",
    "        #subset = data[start:end]                                                \n",
    "        yield train_indices,test_indices\n",
    "\n",
    "cv = K_fold(n_splits=5, seed=42,data=ratings,shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [149989 416292 683230 ... 131932 671155 121958] [895536 899739  55687 ... 477775 424188 293600]\n",
      "1 [895536 899739  55687 ... 131932 671155 121958] [149989 416292 683230 ... 180197 451837 259541]\n",
      "2 [895536 899739  55687 ... 131932 671155 121958] [  8213 946406 997350 ...  66898 559054 969525]\n",
      "3 [895536 899739  55687 ... 131932 671155 121958] [783546 277503 820674 ... 829520  58689 100373]\n",
      "4 [895536 899739  55687 ... 829520  58689 100373] [607117 537829 803592 ... 131932 671155 121958]\n"
     ]
    }
   ],
   "source": [
    "#Testing the K_fold script.\n",
    "for i, (train,test) in enumerate(cv):\n",
    "    print(i, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global mean rating: 3.581564453029317\n",
      "1.1171002038673066\n",
      "0.9338608614369477\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Global Mean Rating: Global mean is the average rating of all the movie ratings in our dataset.\n",
    "This term acts as the baseline prediction rating for all movies and users.\n",
    "'''\n",
    "\n",
    "global_mean_rating = ratings['Rating'].mean()   #Fallback value for recommendation\n",
    "print('Global mean rating = ', global_mean_rating)\n",
    "\n",
    "'''\n",
    "Any movie rating that is greater than the global mean rating value can be recommended to the user\n",
    "First split the data using custom K_fold split script.\n",
    "Check for the rating and compare it with the global mean rating\n",
    "Calculate the mse and mae\n",
    "'''\n",
    "\n",
    "# Create a simple recommendation model that recommends movies based on global mean ratings.\n",
    "def recommend_movies_by_global_mean_ratings(user_id, num_recommendations=10):\n",
    "    # Filter movies with a rating greater than or equal to the global mean.\n",
    "    recommended_movies = [movie_id for movie_id in ratings[\"MovieID\"] if  ratings[\"Rating\"]>=global_mean_rating][:num_recommendations]\n",
    "    return recommended_movies\n",
    "\n",
    "# Function to calculate Root Mean Square Error (RMSE).\n",
    "# RMSE measures the root squared of the mean of the error between predicted and actual ratings.\n",
    "def calculate_rmse(predictions, actual):\n",
    "    mse = mean_squared_error(actual, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "# Function to calculate Mean Absolute Error (MAE).\n",
    "# MAE measures the average absolute difference between predicted and actual ratings.\n",
    "def calculate_mae(predictions, actual):\n",
    "    mae = mean_absolute_error(actual, predictions)\n",
    "    return mae\n",
    "\n",
    "# Lists to store RMSE and MAE scores for each fold\n",
    "rmse_scores = []\n",
    "mae_scores = []\n",
    "\n",
    "# Split the data using K-fold cross-validation script\n",
    "cv = K_fold(n_splits=5, seed=42, data=ratings, shuffle=True)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(cv):\n",
    "    # Split the data into training and testing sets for the current fold.\n",
    "    train_data, test_data = ratings.iloc[train_index], ratings.iloc[test_index]\n",
    "    \n",
    "    y_true = test_data['Rating']\n",
    "    y_pred = [global_mean_rating] * len(test_data[\"Rating\"])\n",
    "\n",
    "     # Calculate RMSE and MAE for the fold and store the score\n",
    "    rmse = calculate_rmse(y_pred, y_true)\n",
    "    rmse_scores.append(rmse)\n",
    "\n",
    "    mae = calculate_mae(y_pred, y_true)\n",
    "    mae_scores.append(mae)\n",
    "\n",
    "#The results for the average RMSE and MAE scores over all folds   \n",
    "print('The root mean squared error is = ', np.average(rmse_scores))\n",
    "print('The mean absolute error score is = ', np.average(mae_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   UserID  MovieID  Rating  Timestamp  Rating_mean\n",
      "0       1     1193       5  978300760     4.390725\n",
      "1       2     1193       5  978298413     4.390725\n",
      "The root mean squared error is =  0.974876592847774\n",
      "The mean absolute error score is =  0.7789571731412801\n"
     ]
    }
   ],
   "source": [
    "# Create a simple recommendation model based on item mean ratings\n",
    "# Calculate item mean ratings for each movie\n",
    "movie_means = ratings.groupby('MovieID')['Rating'].mean().reset_index()\n",
    "df = ratings.merge(movie_means, on='MovieID', suffixes=('', '_mean')) # Merge the movie mean ratings with the original ratings dataset\n",
    "print(df.head(2))\n",
    "\n",
    "# Function to recommend movies based on movie mean ratings\n",
    "def recommend_movies_by_movie_mean_ratings(movie_id, train_data):\n",
    "    pred = []\n",
    "    movie_ids_set = set(train_data[\"MovieID\"]) #We create a set to reduce the search time.\n",
    "    \n",
    "    for i in movie_id:\n",
    "         # Check if the movie ID is in the training dataset else use the global mean rating as a fallback.\n",
    "        if i in movie_ids_set:\n",
    "            prediction = (train_data.loc[train_data['MovieID'] == i, 'Rating_mean'].iloc[0])\n",
    "        else:\n",
    "            prediction = (global_mean_rating)\n",
    "        # Round predicted ratings within the valid range [1, 5]\n",
    "        pred.append(max(1, min(5, prediction)))\n",
    "    return pred\n",
    "\n",
    "# Lists to store RMSE and MAE scores for each fold\n",
    "rmse_scores = []\n",
    "mae_scores = []\n",
    "\n",
    "# Split the data using K-fold cross-validation script\n",
    "cv = K_fold(n_splits=5, seed=42, data=df, shuffle=True)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(cv):\n",
    "    # Split the data into training and testing sets for the current fold.\n",
    "    train_data, test_data = df.iloc[train_index], df.iloc[test_index]\n",
    "\n",
    "    y_true =  test_data['Rating']\n",
    "    y_pred = recommend_movies_by_movie_mean_ratings(test_data[\"MovieID\"], train_data)  #Generate predicted ratings based on movie mean ratings\n",
    "\n",
    "    # Calculate RMSE and MAE for the fold and store the score\n",
    "    rmse = calculate_rmse(y_pred, y_true)\n",
    "    rmse_scores.append(rmse)\n",
    "\n",
    "    mae = calculate_mae(y_pred, y_true)\n",
    "    mae_scores.append(mae)\n",
    "    \n",
    "# The results for the average RMSE and MAE scores over all folds\n",
    "print('The root mean squared error is = ', np.average(rmse_scores))\n",
    "print('The mean absolute error score is = ', np.average(mae_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   UserID  MovieID  Rating  Timestamp  Rating_mean\n",
      "0       1     1193       5  978300760     4.188679\n",
      "1       1      661       3  978302109     4.188679\n",
      "The root mean squared error is =  1.0284465595659256\n",
      "The mean absolute error score is =  0.8234169238414688\n"
     ]
    }
   ],
   "source": [
    "# Create a simple recommendation model based on user mean ratings\n",
    "# Calculate user mean ratings for each movie\n",
    "user_means = ratings.groupby('UserID')['Rating'].mean().reset_index()\n",
    "df = ratings.merge(user_means, on='UserID', suffixes=('', '_mean')) # Merge the user mean ratings with the original ratings dataset\n",
    "print(df.head(2))\n",
    "\n",
    "# Function to recommend movies based on movie mean ratings\n",
    "def recommend_movies_by_user_mean_ratings(user_id, train_data):\n",
    "    pred = []\n",
    "    user_ids_set = set(train_data[\"UserID\"])\n",
    "    \n",
    "    for i in user_id:\n",
    "        if i in user_ids_set:\n",
    "            # Check if the user ID is in the training dataset else use the global mean rating as a fallback.\n",
    "            pred.append(train_data.loc[train_data['UserID'] == i, 'Rating_mean'].iloc[0])\n",
    "        else:\n",
    "            pred.append(global_mean_rating)\n",
    "    return pred\n",
    "\n",
    "# Lists to store RMSE and MAE scores for each fold\n",
    "rmse_scores = []\n",
    "mae_scores = []\n",
    "\n",
    "# Split the data using K-fold cross-validation script\n",
    "cv = K_fold(n_splits=5, seed=42, data=df, shuffle=True)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(cv):\n",
    "    # Split the data into training and testing sets for the current fold.\n",
    "    train_data, test_data = df.iloc[train_index], df.iloc[test_index]\n",
    "\n",
    "    y_true =  test_data['Rating']\n",
    "    y_pred = recommend_movies_by_user_mean_ratings(test_data[\"UserID\"], train_data)  #Generate predicted ratings based on user mean ratings\n",
    "\n",
    "    # Calculate RMSE and MAE for the fold and store the score\n",
    "    rmse = calculate_rmse(y_pred, y_true)\n",
    "    rmse_scores.append(rmse)\n",
    "\n",
    "    mae = calculate_mae(y_pred, y_true)\n",
    "    mae_scores.append(mae)\n",
    "    \n",
    "#The results for the average RMSE and MAE scores over all folds   \n",
    "print('The root mean squared error is = ', np.average(rmse_scores))\n",
    "print('The mean absolute error score is = ', np.average(mae_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   UserID  MovieID  Rating  Timestamp  Rating_movie_mean  Rating_user_mean\n",
      "0       1     1193       5  978300760           4.390725          4.188679\n",
      "1       1      661       3  978302109           3.464762          4.188679\n",
      "The root mean squared error is =  0.9155312414686204\n",
      "The mean absolute error score is =  0.7256333945260849\n"
     ]
    }
   ],
   "source": [
    "# Calculate item mean ratings for each movie\n",
    "movie_means = ratings.groupby('MovieID')['Rating'].mean().reset_index()\n",
    "\n",
    "# Calculate user mean ratings for each user\n",
    "user_means = ratings.groupby('UserID')['Rating'].mean().reset_index()\n",
    "\n",
    "# Merge movie mean ratings with the original ratings dataset\n",
    "df = ratings.merge(movie_means, on='MovieID', suffixes=('', '_movie_mean'))\n",
    "\n",
    "# Merge user mean ratings with the dataset\n",
    "df = df.merge(user_means, on='UserID', suffixes=('', '_user_mean'))\n",
    "print(df.head(2))\n",
    "\n",
    "# Function to get the coefficients for the linear regression model\n",
    "def build_linear_regression_model(train_data):\n",
    "    \"\"\"\n",
    "    Build a linear regression model to estimate alpha, beta, and gamma coefficients.\n",
    "\n",
    "    Parameters:\n",
    "    - train_data: Training data containing 'Rating', 'Rating_movie_mean', and 'Rating_user_mean'.\n",
    "\n",
    "    Returns:\n",
    "    - coefficients: Coefficients [alpha, beta, gamma] estimated by the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a design matrix X with columns: avguser (alpha), avgmovie (beta), and a constant term (1 for gamma)\n",
    "    X = np.column_stack((train_data['Rating_user_mean'], train_data['Rating_movie_mean'], np.ones(len(train_data))))\n",
    "\n",
    "    #NumPy's lstsq function to estimate alpha, beta, and gamma\n",
    "    coefficients, residuals, rank, singular_values = np.linalg.lstsq(X, train_data['Rating'], rcond=None)\n",
    "    return coefficients  # coefficients contains [alpha, beta, gamma]\n",
    "\n",
    "def recommend_movies_by_linear_regression(movie_data, user_data, model):\n",
    "    \"\"\"\n",
    "    Predict using a linear regression model.\n",
    "\n",
    "    Parameters:\n",
    "    - user_avg: Average user rating of test data.\n",
    "    - movie_avgs: Average movie ratings of test data.\n",
    "    - model: Linear regression model coefficients [alpha, beta, gamma].\n",
    "\n",
    "    Returns:\n",
    "    - predictions: Predicted movie ratings based on the model.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    for i, j in zip(movie_data,user_data):\n",
    "        if np.isnan(i) or np.isnan(j):\n",
    "            # Use the global mean rating as the fall-back value when either user or movie data is missing\n",
    "            predictions.append(global_mean_rating)\n",
    "        else:\n",
    "            # Predict ratings using the linear regression model: alpha * user_data + beta * movie_data + gamma\n",
    "            prediction = model[0] * j + model[1] * i + model[2]\n",
    "            # Round predictions to be within the valid range [1, 5]\n",
    "            predictions.append(np.maximum(1, np.minimum(5, prediction)))\n",
    "    return predictions\n",
    "\n",
    "# Lists to store RMSE and MAE scores for each fold\n",
    "rmse_scores = []\n",
    "mae_scores = []\n",
    "\n",
    "# Split the data using K-fold cross-validation script\n",
    "cv = K_fold(n_splits=5, seed=42, data=df, shuffle=True)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(cv):\n",
    "    # Split the data into training and testing sets for the current fold.\n",
    "    train_data, test_data = df.iloc[train_index], df.iloc[test_index]\n",
    "    # Build the regression model to get the coefficients \n",
    "    model = build_linear_regression_model(train_data)\n",
    "\n",
    "    # Make movie recommendations for each user in the test set\n",
    "    y_pred = recommend_movies_by_linear_regression(test_data[\"Rating_movie_mean\"], test_data[\"Rating_user_mean\"], model)\n",
    "    y_true = test_data['Rating']\n",
    "    \n",
    "    # Calculate RMSE and MAE for the fold and store the score\n",
    "    rmse = calculate_rmse(y_pred, y_true)\n",
    "    rmse_scores.append(rmse)\n",
    "\n",
    "    mae = calculate_mae(y_pred,y_true)\n",
    "    mae_scores.append(mae)\n",
    "    \n",
    "# The results for the average RMSE and MAE scores over all folds\n",
    "print('The root mean squared error is = ', np.average(rmse_scores))\n",
    "print('The mean absolute error score is = ', np.average(mae_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**TASK 2 - The UV matrix decomposition**\n",
    "In this task, we used a method called UV-decomposition, which is a dimensionality reduction approach for estimating the missing values in a utility matrix. This approach assumes that the utility matrix $M$ can be represented as the product of two matrices, $U$ as user-related features and $V$ as item-related features.\n",
    "\n",
    "By decomposing the utility matrix $M$ into two matrices, $U$ and $V$, we are aiming to closely approximate $M$ in the non-blank entries. This allows us to estimate the missing values in the $M$ matrix by computing corresponding entries in the product $UV$.\n",
    "\n",
    "The final goal is to find the entries of $U$ and $V$ that best approximate the known values of $M$. However, decomposition may not yield a perfect match for all non-blank entries in $M$, especially when there are more known entries than the total entries in $U$ and $V$ combined.\n",
    "\n",
    "In summary, the decomposition could be outlined in four key areas:\n",
    "- Preprocessing the Matrix, in our case by applying mean centering to the matrix.\n",
    "- Initializing Matrices $U$ and $V$, in our case to numbers one.\n",
    "- Optimizing order of values in U and V matrices, which denote two following notation respectively:\n",
    "$u_{rs}= \\frac{ \\sum_{j}v_{sj}(m_{rj}- \\sum_{k \\neq s}u_{rk}v_{kj})}{ \\sum_{j}v^2_{sj}}$ ; $v_{rs}= \\frac{ \\sum_{i}u_{ir}(m_{is}- \\sum_{k \\neq r}u_{ik}v_{ks})}{ \\sum_{i}u^2_{ir}}$\n",
    "- Ending the attempt at optimization."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "'''\n",
    "Create a utility matrix M from a user-item ratings. The resulting matrix is designed to represent user ratings for items, where missing values are denoted as NaN.\n",
    "\n",
    "Parameters:\n",
    "- ratings: Pandas df with columns 'user', 'movie', and 'rating'.\n",
    "\n",
    "Returns:\n",
    "- M: Utility matrix where missing values are represented as NaN.\n",
    "'''\n",
    "def makeM(ratings):\n",
    "    M = np.empty((6040, 3952))\n",
    "    M[:] = np.nan\n",
    "    for (u,m,r,_) in ratings:\n",
    "        M[u-1,m-1] = r\n",
    "    return M\n",
    "\n",
    "\n",
    "'''\n",
    "Function perform an update of the U matrix (part of UV-decomposition), based on the V matrix and the original utility matrix M.\n",
    "\n",
    "Parameters:\n",
    "- U: The U matrix\n",
    "- V: The V matrix\n",
    "- M: The original M matrix\n",
    "- n: The number of user\n",
    "- f: The number of features\n",
    "'''\n",
    "def decomposeU(U,V,M,n,f):\n",
    "    for r in range(n):\n",
    "        for s in range(f):\n",
    "            m_array = np.array(M[r,:])\n",
    "            v_array=np.array(V[s,:])\n",
    "            v_array[np.isnan(m_array)]=np.nan\n",
    "            denominator=np.nansum(np.square(v_array))\n",
    "            if denominator == 0:\n",
    "                denominator = 1\n",
    "            sum_array=np.matmul(U[r,:],V[:])-(U[r,s]*V[s,:])\n",
    "            numerator=np.nansum(V[s,:]*(m_array-sum_array))\n",
    "            U[r,s]=numerator/denominator\n",
    "\n",
    "\n",
    "'''\n",
    "Function perform an update of the V matrix (part of UV-decomposition), based on the U matrix and the original utility matrix M.\n",
    "\n",
    "Parameters:\n",
    "- U: The U matrix\n",
    "- V: The V matrix\n",
    "- M: The original M matrix\n",
    "- m: The number of movies\n",
    "- f: The number of features\n",
    "'''\n",
    "def decomposeV(U,V,M,m,f):\n",
    "    for s in range(0,m):\n",
    "        for r in range(f):\n",
    "            m_array=np.array(M[:,s])\n",
    "            u_array=np.array(U[:,r])\n",
    "            u_array[np.isnan(m_array)]=np.nan\n",
    "            denominator=np.nansum(np.square(u_array))\n",
    "            if denominator == 0:\n",
    "                denominator = 1\n",
    "            sum_array = np.matmul(U[:], V[:,s]) - (V[r, s] * U[:, r])\n",
    "            numerator = np.nansum(U[:, r] * (m_array - sum_array))\n",
    "            V[r,s]=numerator/denominator\n",
    "\n",
    "'''\n",
    "RMSE is used to measure the accuracy of predictive model, in our case in context of UV-decomposition and Matrix Factorization. MSE quantifies the average magnitude of the errors or the discrepancies between the actual and predicted values in a utility matrix M, where lower RMSE values indicate a more accurate model.\n",
    "'''\n",
    "def calculate_rmse(matrix, test_data):\n",
    "    pred = []\n",
    "    actual = []\n",
    "    for (user_id, movie_id, rating, _) in test_data:\n",
    "        pred.append(matrix[user_id-1, movie_id-1])\n",
    "        actual.append(rating)\n",
    "    mae_value = mean_absolute_error(actual, pred)\n",
    "    rmse_value = np.sqrt(mean_squared_error(actual, pred))\n",
    "    return rmse_value, mae_value\n",
    "\n",
    "\n",
    "'''\n",
    "Function is capable of preprocessing a given utility matrix by applying a mean centering technique. This approach calculates and subtracts user and item averages to center the data around zero.\n",
    "\n",
    "Parameters:\n",
    "- M: Utility matrix, with blank entries containing missing values (NaN).\n",
    "\n",
    "Returns:\n",
    "- M: Normalized matrix that has been adjusted to have a center or mean of zero, removing biases.\n",
    "- user_average: Matrix storing the calculated average user ratings, where missing values are replaced with zeros.\n",
    "- movie_average: A matrix storing the calculated average item ratings, where missing values are replaced with zeros.\n",
    "'''\n",
    "def normalize_M(M):\n",
    "    user_average = np.empty(M.shape)\n",
    "    movie_average = np.empty(M.shape)\n",
    "    n, m = M.shape\n",
    "\n",
    "    for i in range(n):\n",
    "        row = M[i, :]\n",
    "        if np.any(~np.isnan(row)):\n",
    "            user_average[i,:] = np.nanmean(row)\n",
    "        else:\n",
    "            user_average[i, :] = 0\n",
    "    for j in range(m):\n",
    "        col = M[:, j]\n",
    "        if np.any(~np.isnan(col)):\n",
    "            movie_average[:,j] = np.nanmean(col)\n",
    "        else:\n",
    "            movie_average[:, j] = 0\n",
    "\n",
    "    M = M - 0.5*user_average - 0.5*movie_average\n",
    "    return M, user_average, movie_average"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE list: [0.7240876673484067, 0.7183742510197288, 0.7185300144505843, 0.7196128560955047, 0.7201305838591022]\n",
      "Average MAE: 0.7201470745546653\n",
      "RMSE list: [0.9164468022700587, 0.9110241978208942, 0.9106784848578212, 0.9120150742493378, 0.9135895904985155]\n",
      "Average RMSE: 0.9127508299393255\n"
     ]
    }
   ],
   "source": [
    "cv = K_fold(n_splits=5, seed=42, data=ratings, shuffle=True)\n",
    "\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(cv):\n",
    "    train_data, test_data = ratings.iloc[train_index], ratings.iloc[test_index]\n",
    "    M = makeM(train_data.to_numpy()) # Create a utility matrix M from the training data\n",
    "    n,m = M.shape # Get the number of users (n) and items (m) in the utility matrix\n",
    "    f = 2 # Set the number of features (f) to 2\n",
    "\n",
    "    # Initialize matrices U and V with ones, representing user and item features\n",
    "    U = np.ones((n, f))\n",
    "    V = np.ones((f, m))\n",
    "    M, user_avg, movie_avg = normalize_M(M)\n",
    "\n",
    "    # Optimize matrix U and V using collaborative filtering and the normalized utility matrix M\n",
    "    decomposeU(U,V,M,n,f)\n",
    "    decomposeV(U,V,M,m,f)\n",
    "\n",
    "    # Compute the predicted utility matrix UV and add back the mean-centered user and item averages\n",
    "    UV = np.dot(U,V) + 0.5*user_avg + 0.5*movie_avg\n",
    "\n",
    "    # Calculate the RMSE (Root Mean Square Error) and MSE between the predicted UV and the test data\n",
    "    rmse_val, mae_val = calculate_rmse(UV, test_data.to_numpy())\n",
    "    rmse_list.append(rmse_val)\n",
    "    mae_list.append(mae_val)\n",
    "\n",
    "print(\"MAE list:\", mae_list)\n",
    "print(\"Average MAE:\", sum(mae_list) / len(mae_list))\n",
    "print(\"RMSE list:\", rmse_list)\n",
    "print(\"Average RMSE:\", sum(rmse_list) / len(rmse_list))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**TASK 3 - The Matrix Factorization**\n",
    "\n",
    "In this subtask, we have tackled the MovieLens 1M dataset problem with Matrix Factorization approach presented in research paper by Gábor Takács et al. MF is a fundamental and yet powerful technique used in collaborative filtering to approximate a user-item matrix as the product of two matrices.\n",
    "\n",
    "In our case we denoted the user-movie matrix as $X$ and $U$, $M$ for users and movies respectively:\n",
    "- $U = I $ x $ K$\n",
    "- $M = K $ x $ J$\n",
    "\n",
    "For the given problem, where $X$ has many unknown elements, the goal is to find $U$ and $M$ that minimize the sum of squared errors (RMSE) on known elements of $X$.\n",
    "The optimization process involves gradient descent to update $U$ and $M$ matrices based on calculated error along with defined regularization parameter.\n",
    "\n",
    "Whole algorithm could be described as a set of following steps:\n",
    "1. Initialize $U$ and $M$ randomly with small positive values, in our case numbers spanning between 0 and 1. Additionally, we settle the learning rate $λ$ to 0.005 and regularization factor $η$ to 0.05.\n",
    "2. Iterate, for our purposes - 75 times, with optional condition to terminate when RMSE doesn't decrease during two next iterations:\n",
    "- Iterate over each known element of X not in the probe subset:\n",
    "i. Compute the error, where $e_{ij} = x_{ij} - \\hat{x}_{ij} \\text{ for } (i; j) \\in R$\n",
    "ii. Compute the gradient of the error = $e_{ij}^2$, which can be denoted as:\n",
    "$\\frac{∂}{∂u_{ik}}e_{ij}^2 = -2e_{ij} * m_{kj}$ ; $\\frac{∂}{∂m_{kj}}e_{ij}^2 = -2e_{ij} * m_{ik}$\n",
    "iii. Update the ith row of U and the jth column of M, with respect to following equations:\n",
    "$u'_{ik} = u_{ik} + \\lambda \\cdot (2e_{ij} \\cdot m_{kj} - \\eta \\cdot u_{ik})$ ; $m'_{kj} = m_{kj} + \\lambda \\cdot (2e_{ij} \\cdot u_{ik} - \\eta \\cdot m_{kj})$\n",
    "- Finally, calculate the RMSE on the probe subset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "'''\n",
    "Create a user-movie rating matrix from a list of ratings, where rows represent users,\n",
    "columns represent movies, and each cell contains the corresponding user's rating for movie.\n",
    "\n",
    "Parameters:\n",
    "- ratings: Pandas df with columns 'user', 'movie', and 'rating'.\n",
    "\n",
    "Returns:\n",
    "- X: User-movie rating matrix where X[i, j] represents the rating of user i for movie j.\n",
    "'''\n",
    "def makeX(ratings):\n",
    "    X = np.empty((6040, 3952))\n",
    "    M[:] = 0 # Initialize X with zeros\n",
    "    for (u,m,r,_) in ratings:\n",
    "        M[u-1,m-1] = r # Fill in the matrix X with known user rating\n",
    "    return X"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "RMSE: 0.9395865772397065\n",
      "MAE: 0.7455756351852879\n",
      "Iteration: 1\n",
      "RMSE: 0.9317407850122721\n",
      "MAE: 0.7386081457396757\n",
      "Iteration: 2\n",
      "RMSE: 0.9290093550298499\n",
      "MAE: 0.736191119072965\n",
      "Iteration: 3\n",
      "RMSE: 0.9259107988370959\n",
      "MAE: 0.733560039735887\n",
      "Iteration: 4\n",
      "RMSE: 0.9208048806766637\n",
      "MAE: 0.7291634101200668\n",
      "Iteration: 5\n",
      "RMSE: 0.9147143422418723\n",
      "MAE: 0.7236768650968995\n",
      "Iteration: 6\n",
      "RMSE: 0.9093540206004179\n",
      "MAE: 0.718747376390171\n",
      "Iteration: 7\n",
      "RMSE: 0.9049227591453665\n",
      "MAE: 0.7146397601560858\n",
      "Iteration: 8\n",
      "RMSE: 0.9011702143183328\n",
      "MAE: 0.711168682662196\n",
      "Iteration: 9\n",
      "RMSE: 0.8979411293823012\n",
      "MAE: 0.7081285902254577\n",
      "Iteration: 10\n",
      "RMSE: 0.8951566130148998\n",
      "MAE: 0.705484729609081\n",
      "Iteration: 11\n",
      "RMSE: 0.8927563209818494\n",
      "MAE: 0.703181561690356\n",
      "Iteration: 12\n",
      "RMSE: 0.8906824542259497\n",
      "MAE: 0.7011708725727439\n",
      "Iteration: 13\n",
      "RMSE: 0.8888827554423918\n",
      "MAE: 0.6994007780097301\n",
      "Iteration: 14\n",
      "RMSE: 0.8873128819167571\n",
      "MAE: 0.6978475593503194\n",
      "Iteration: 15\n",
      "RMSE: 0.8859360008845675\n",
      "MAE: 0.6964868618094879\n",
      "Iteration: 16\n",
      "RMSE: 0.8847215878043464\n",
      "MAE: 0.6952822507953321\n",
      "Iteration: 17\n",
      "RMSE: 0.8836443418429678\n",
      "MAE: 0.694217342509482\n",
      "Iteration: 18\n",
      "RMSE: 0.8826833247071615\n",
      "MAE: 0.6932661015407511\n",
      "Iteration: 19\n",
      "RMSE: 0.8818212473270813\n",
      "MAE: 0.692415343195833\n",
      "Iteration: 20\n",
      "RMSE: 0.8810438546602919\n",
      "MAE: 0.6916521769173526\n",
      "Iteration: 21\n",
      "RMSE: 0.8803393956567819\n",
      "MAE: 0.6909601397392527\n",
      "Iteration: 22\n",
      "RMSE: 0.8796981765915023\n",
      "MAE: 0.6903369992640572\n",
      "Iteration: 23\n",
      "RMSE: 0.8791121925796045\n",
      "MAE: 0.6897694134031285\n",
      "Iteration: 24\n",
      "RMSE: 0.8785748267222268\n",
      "MAE: 0.6892531586620724\n",
      "Iteration: 25\n",
      "RMSE: 0.8780806040430846\n",
      "MAE: 0.6887732611808722\n",
      "Iteration: 26\n",
      "RMSE: 0.8776249882385343\n",
      "MAE: 0.68832789459397\n",
      "Iteration: 27\n",
      "RMSE: 0.877204211746978\n",
      "MAE: 0.687916225190269\n",
      "Iteration: 28\n",
      "RMSE: 0.8768151323761278\n",
      "MAE: 0.6875351144550437\n",
      "Iteration: 29\n",
      "RMSE: 0.8764551119640163\n",
      "MAE: 0.6871829675033818\n",
      "Iteration: 30\n",
      "RMSE: 0.8761219140704501\n",
      "MAE: 0.6868542786103967\n",
      "Iteration: 31\n",
      "RMSE: 0.8758136185707759\n",
      "MAE: 0.6865489535339636\n",
      "Iteration: 32\n",
      "RMSE: 0.8755285514336699\n",
      "MAE: 0.6862646903796717\n",
      "Iteration: 33\n",
      "RMSE: 0.8752652280953928\n",
      "MAE: 0.6859993984411631\n",
      "Iteration: 34\n",
      "RMSE: 0.8750223088428041\n",
      "MAE: 0.6857532213279124\n",
      "Iteration: 35\n",
      "RMSE: 0.8747985645866843\n",
      "MAE: 0.6855267484543558\n",
      "Iteration: 36\n",
      "RMSE: 0.874592851402728\n",
      "MAE: 0.6853172398605281\n",
      "Iteration: 37\n",
      "RMSE: 0.8744040922646075\n",
      "MAE: 0.6851230151987575\n",
      "Iteration: 38\n",
      "RMSE: 0.8742312644946646\n",
      "MAE: 0.6849448236261647\n",
      "Iteration: 39\n",
      "RMSE: 0.874073391603484\n",
      "MAE: 0.6847810811715058\n",
      "Iteration: 40\n",
      "RMSE: 0.8739295383646984\n",
      "MAE: 0.6846304753099665\n",
      "Iteration: 41\n",
      "RMSE: 0.8737988081598298\n",
      "MAE: 0.6844902363882329\n",
      "Iteration: 42\n",
      "RMSE: 0.8736803418155121\n",
      "MAE: 0.6843625524782007\n",
      "Iteration: 43\n",
      "RMSE: 0.8735733173310005\n",
      "MAE: 0.68424815988729\n",
      "Iteration: 44\n",
      "RMSE: 0.8734769500499532\n",
      "MAE: 0.6841441905812986\n",
      "Iteration: 45\n",
      "RMSE: 0.8733904929629437\n",
      "MAE: 0.6840500608937761\n",
      "Iteration: 46\n",
      "RMSE: 0.8733132369347807\n",
      "MAE: 0.6839655373725296\n",
      "Iteration: 47\n",
      "RMSE: 0.8732445107344148\n",
      "MAE: 0.6838887544849436\n",
      "Iteration: 48\n",
      "RMSE: 0.8731836808073511\n",
      "MAE: 0.6838200615367631\n",
      "Iteration: 49\n",
      "RMSE: 0.8731301507742844\n",
      "MAE: 0.6837577434400988\n",
      "Iteration: 50\n",
      "RMSE: 0.8730833606685664\n",
      "MAE: 0.6837020390621351\n",
      "Iteration: 51\n",
      "RMSE: 0.8730427859424831\n",
      "MAE: 0.6836514119773353\n",
      "Iteration: 52\n",
      "RMSE: 0.8730079362811646\n",
      "MAE: 0.6836041767854875\n",
      "Iteration: 53\n",
      "RMSE: 0.872978354265821\n",
      "MAE: 0.6835618118949821\n",
      "Iteration: 54\n",
      "RMSE: 0.8729536139269177\n",
      "MAE: 0.6835223001140679\n",
      "Iteration: 55\n",
      "RMSE: 0.8729333192244526\n",
      "MAE: 0.6834858445974261\n",
      "Iteration: 56\n",
      "RMSE: 0.8729171024878037\n",
      "MAE: 0.6834521087246863\n",
      "Iteration: 57\n",
      "RMSE: 0.8729046228424657\n",
      "MAE: 0.6834213011436683\n",
      "Iteration: 58\n",
      "RMSE: 0.8728955646459373\n",
      "MAE: 0.6833941523994962\n",
      "Iteration: 59\n",
      "RMSE: 0.8728896359503372\n",
      "MAE: 0.68337149276663\n",
      "Iteration: 60\n",
      "RMSE: 0.8728865670052014\n",
      "MAE: 0.6833504509148836\n",
      "Iteration: 61\n",
      "RMSE: 0.8728861088104094\n",
      "MAE: 0.6833308256612182\n",
      "Iteration: 62\n",
      "RMSE: 0.8728880317262813\n",
      "MAE: 0.6833146457770278\n",
      "Iteration: 63\n",
      "RMSE: 0.8728921241455324\n",
      "MAE: 0.6833021401560625\n",
      "Iteration: 64\n",
      "RMSE: 0.8728981912299243\n",
      "MAE: 0.6832918514233471\n",
      "Iteration: 65\n",
      "RMSE: 0.8729060537129862\n",
      "MAE: 0.683282437926426\n",
      "Iteration: 66\n",
      "RMSE: 0.8729155467690766\n",
      "MAE: 0.6832748130600439\n",
      "Iteration: 67\n",
      "RMSE: 0.8729265189481962\n",
      "MAE: 0.6832686984912997\n",
      "Iteration: 68\n",
      "RMSE: 0.8729388311753242\n",
      "MAE: 0.6832643429248791\n",
      "Iteration: 69\n",
      "RMSE: 0.872952355812572\n",
      "MAE: 0.6832621107673371\n",
      "Iteration: 70\n",
      "RMSE: 0.8729669757820865\n",
      "MAE: 0.6832610983991037\n",
      "Iteration: 71\n",
      "RMSE: 0.8729825837473805\n",
      "MAE: 0.6832619122082453\n",
      "Iteration: 72\n",
      "RMSE: 0.8729990813505775\n",
      "MAE: 0.6832638857236579\n",
      "Iteration: 73\n",
      "RMSE: 0.8730163785029355\n",
      "MAE: 0.6832661065528348\n",
      "Iteration: 74\n",
      "RMSE: 0.8730343927259353\n",
      "MAE: 0.6832686411084417\n",
      "Iteration: 0\n",
      "RMSE: 0.9406679125705667\n",
      "MAE: 0.7460216484103602\n",
      "Iteration: 1\n",
      "RMSE: 0.9327208448821509\n",
      "MAE: 0.7390101222379372\n",
      "Iteration: 2\n",
      "RMSE: 0.92937896274167\n",
      "MAE: 0.7361390670433415\n",
      "Iteration: 3\n",
      "RMSE: 0.9251265145749611\n",
      "MAE: 0.7325279624601296\n",
      "Iteration: 4\n",
      "RMSE: 0.9188241599273701\n",
      "MAE: 0.7270536983423851\n",
      "Iteration: 5\n",
      "RMSE: 0.9123215967617476\n",
      "MAE: 0.7212094947093914\n",
      "Iteration: 6\n",
      "RMSE: 0.906910636522119\n",
      "MAE: 0.7162859623438009\n",
      "Iteration: 7\n",
      "RMSE: 0.9023749386852105\n",
      "MAE: 0.7121345949469553\n",
      "Iteration: 8\n",
      "RMSE: 0.8984198537570062\n",
      "MAE: 0.7084825818062128\n",
      "Iteration: 9\n",
      "RMSE: 0.8949452634420073\n",
      "MAE: 0.7052520915095676\n",
      "Iteration: 10\n",
      "RMSE: 0.8919307588248918\n",
      "MAE: 0.7024234311855605\n",
      "Iteration: 11\n",
      "RMSE: 0.8893524653911025\n",
      "MAE: 0.6999778164092738\n",
      "Iteration: 12\n",
      "RMSE: 0.8871657200075769\n",
      "MAE: 0.6978803616640252\n",
      "Iteration: 13\n",
      "RMSE: 0.8853157735212759\n",
      "MAE: 0.696102968505346\n",
      "Iteration: 14\n",
      "RMSE: 0.8837493057978622\n",
      "MAE: 0.6945987473783695\n",
      "Iteration: 15\n",
      "RMSE: 0.8824200495708698\n",
      "MAE: 0.6933104323746447\n",
      "Iteration: 16\n",
      "RMSE: 0.8812899293978234\n",
      "MAE: 0.6921941387949822\n",
      "Iteration: 17\n",
      "RMSE: 0.8803280744838661\n",
      "MAE: 0.6912297616875271\n",
      "Iteration: 18\n",
      "RMSE: 0.8795092644374181\n",
      "MAE: 0.6903996899484798\n",
      "Iteration: 19\n",
      "RMSE: 0.8788125410662735\n",
      "MAE: 0.6896915894744415\n",
      "Iteration: 20\n",
      "RMSE: 0.8782201950050176\n",
      "MAE: 0.6890818875032577\n",
      "Iteration: 21\n",
      "RMSE: 0.8777170896651425\n",
      "MAE: 0.6885576436604606\n",
      "Iteration: 22\n",
      "RMSE: 0.8772902148263053\n",
      "MAE: 0.6881063975383178\n",
      "Iteration: 23\n",
      "RMSE: 0.8769283737348348\n",
      "MAE: 0.6877147604854092\n",
      "Iteration: 24\n",
      "RMSE: 0.8766219414398684\n",
      "MAE: 0.6873779597670043\n",
      "Iteration: 25\n",
      "RMSE: 0.8763626616155996\n",
      "MAE: 0.6870879148488962\n",
      "Iteration: 26\n",
      "RMSE: 0.8761434675904818\n",
      "MAE: 0.6868350011779499\n",
      "Iteration: 27\n",
      "RMSE: 0.875958322498969\n",
      "MAE: 0.6866150334662783\n",
      "Iteration: 28\n",
      "RMSE: 0.8758020770233895\n",
      "MAE: 0.686423418291471\n",
      "Iteration: 29\n",
      "RMSE: 0.8756703440172463\n",
      "MAE: 0.686254879560316\n",
      "Iteration: 30\n",
      "RMSE: 0.8755593891218417\n",
      "MAE: 0.6861082932368582\n",
      "Iteration: 31\n",
      "RMSE: 0.8754660361250218\n",
      "MAE: 0.6859784695406089\n",
      "Iteration: 32\n",
      "RMSE: 0.8753875855614088\n",
      "MAE: 0.685867456242089\n",
      "Iteration: 33\n",
      "RMSE: 0.8753217449700871\n",
      "MAE: 0.6857671358667129\n",
      "Iteration: 34\n",
      "RMSE: 0.8752665692717866\n",
      "MAE: 0.6856750196859399\n",
      "Iteration: 35\n",
      "RMSE: 0.8752204098501271\n",
      "MAE: 0.6855909203498571\n",
      "Iteration: 36\n",
      "RMSE: 0.8751818710775036\n",
      "MAE: 0.6855140634293795\n",
      "Iteration: 37\n",
      "RMSE: 0.8751497731880946\n",
      "MAE: 0.6854443691089042\n",
      "Iteration: 38\n",
      "RMSE: 0.8751231205534404\n",
      "MAE: 0.6853801833886591\n",
      "Iteration: 39\n",
      "RMSE: 0.8751010745536739\n",
      "MAE: 0.6853195016353525\n",
      "Iteration: 40\n",
      "RMSE: 0.8750829303581571\n",
      "MAE: 0.6852639083601875\n",
      "Iteration: 41\n",
      "RMSE: 0.8750680970336845\n",
      "MAE: 0.6852112681573835\n",
      "Iteration: 42\n",
      "RMSE: 0.8750560804880573\n",
      "MAE: 0.6851627652711435\n",
      "Iteration: 43\n",
      "RMSE: 0.8750464688334451\n",
      "MAE: 0.6851185416107887\n",
      "Iteration: 44\n",
      "RMSE: 0.8750389198190784\n",
      "MAE: 0.6850796046449955\n",
      "Iteration: 45\n",
      "RMSE: 0.8750331500378957\n",
      "MAE: 0.6850437288967491\n",
      "Iteration: 46\n",
      "RMSE: 0.8750289256580454\n",
      "MAE: 0.6850097095340151\n",
      "Iteration: 47\n",
      "RMSE: 0.8750260544687751\n",
      "MAE: 0.6849767372868709\n",
      "Iteration: 48\n",
      "RMSE: 0.8750243790622798\n",
      "MAE: 0.6849443713783421\n",
      "Iteration: 49\n",
      "RMSE: 0.8750237709995384\n",
      "MAE: 0.6849135171060088\n",
      "Iteration: 50\n",
      "RMSE: 0.8750241258299475\n",
      "MAE: 0.6848844625742315\n",
      "Iteration: 51\n",
      "RMSE: 0.875025358852487\n",
      "MAE: 0.6848566344121686\n",
      "Iteration: 52\n",
      "RMSE: 0.8750274015209817\n",
      "MAE: 0.6848304786636781\n",
      "Iteration: 53\n",
      "RMSE: 0.875030198408369\n",
      "MAE: 0.6848063803831038\n",
      "Iteration: 54\n",
      "RMSE: 0.8750337046552761\n",
      "MAE: 0.6847844800252543\n",
      "Iteration: 55\n",
      "RMSE: 0.8750378838371076\n",
      "MAE: 0.6847634285978778\n",
      "Iteration: 56\n",
      "RMSE: 0.8750427061915375\n",
      "MAE: 0.6847420150073982\n",
      "Iteration: 57\n",
      "RMSE: 0.8750481471550946\n",
      "MAE: 0.6847206386638779\n",
      "Iteration: 58\n",
      "RMSE: 0.8750541861635727\n",
      "MAE: 0.6847004304559849\n",
      "Iteration: 59\n",
      "RMSE: 0.8750608056764458\n",
      "MAE: 0.6846815191314055\n",
      "Iteration: 60\n",
      "RMSE: 0.8750679903903998\n",
      "MAE: 0.6846626282571572\n",
      "Iteration: 61\n",
      "RMSE: 0.8750757266115596\n",
      "MAE: 0.684643845724088\n",
      "Iteration: 62\n",
      "RMSE: 0.8750840017600535\n",
      "MAE: 0.6846259860997843\n",
      "Iteration: 63\n",
      "RMSE: 0.8750928039842031\n",
      "MAE: 0.6846081069981625\n",
      "Iteration: 64\n",
      "RMSE: 0.8751021218649125\n",
      "MAE: 0.6845915157558915\n",
      "Iteration: 65\n",
      "RMSE: 0.8751119441937507\n",
      "MAE: 0.6845751635250958\n",
      "Iteration: 66\n",
      "RMSE: 0.8751222598107884\n",
      "MAE: 0.6845584377413375\n",
      "Iteration: 67\n",
      "RMSE: 0.8751330574905145\n",
      "MAE: 0.6845413082751282\n",
      "Iteration: 68\n",
      "RMSE: 0.875144325866096\n",
      "MAE: 0.6845236717723939\n",
      "Iteration: 69\n",
      "RMSE: 0.8751560533839292\n",
      "MAE: 0.6845061804123935\n",
      "Iteration: 70\n",
      "RMSE: 0.8751682282818467\n",
      "MAE: 0.684489170906377\n",
      "Iteration: 71\n",
      "RMSE: 0.8751808385855578\n",
      "MAE: 0.6844724253418563\n",
      "Iteration: 72\n",
      "RMSE: 0.8751938721188938\n",
      "MAE: 0.6844560510106625\n",
      "Iteration: 73\n",
      "RMSE: 0.875207316524277\n",
      "MAE: 0.6844406694788869\n",
      "Iteration: 74\n",
      "RMSE: 0.8752211592905211\n",
      "MAE: 0.6844264088019703\n",
      "Iteration: 0\n",
      "RMSE: 0.9406013052333243\n",
      "MAE: 0.7467622724268632\n",
      "Iteration: 1\n",
      "RMSE: 0.9332638786180167\n",
      "MAE: 0.7401616306018072\n",
      "Iteration: 2\n",
      "RMSE: 0.9306147857035405\n",
      "MAE: 0.737734130089795\n",
      "Iteration: 3\n",
      "RMSE: 0.9274708164717627\n",
      "MAE: 0.734905968749553\n",
      "Iteration: 4\n",
      "RMSE: 0.9222981651720744\n",
      "MAE: 0.7302536695053513\n",
      "Iteration: 5\n",
      "RMSE: 0.9161917967036839\n",
      "MAE: 0.724700160577124\n",
      "Iteration: 6\n",
      "RMSE: 0.9107591281097153\n",
      "MAE: 0.7197331672242134\n",
      "Iteration: 7\n",
      "RMSE: 0.9060765832820675\n",
      "MAE: 0.7154789679152582\n",
      "Iteration: 8\n",
      "RMSE: 0.9018929545184698\n",
      "MAE: 0.7116999279576276\n",
      "Iteration: 9\n",
      "RMSE: 0.898149378213561\n",
      "MAE: 0.7083000973281526\n",
      "Iteration: 10\n",
      "RMSE: 0.894879638438454\n",
      "MAE: 0.7053019380435918\n",
      "Iteration: 11\n",
      "RMSE: 0.8920911413782548\n",
      "MAE: 0.7027571241718465\n",
      "Iteration: 12\n",
      "RMSE: 0.8897420261896283\n",
      "MAE: 0.7005919692314243\n",
      "Iteration: 13\n",
      "RMSE: 0.887764656986743\n",
      "MAE: 0.6987420715037638\n",
      "Iteration: 14\n",
      "RMSE: 0.8860888654585318\n",
      "MAE: 0.6971380467796324\n",
      "Iteration: 15\n",
      "RMSE: 0.884653731791237\n",
      "MAE: 0.6957424062462549\n",
      "Iteration: 16\n",
      "RMSE: 0.883410907917145\n",
      "MAE: 0.6945174003567701\n",
      "Iteration: 17\n",
      "RMSE: 0.8823237719768432\n",
      "MAE: 0.693432233075207\n",
      "Iteration: 18\n",
      "RMSE: 0.8813651304391626\n",
      "MAE: 0.6924822023185174\n",
      "Iteration: 19\n",
      "RMSE: 0.8805147944949372\n",
      "MAE: 0.6916487250548462\n",
      "Iteration: 20\n",
      "RMSE: 0.879757545310578\n",
      "MAE: 0.6909032150439798\n",
      "Iteration: 21\n",
      "RMSE: 0.8790816045172287\n",
      "MAE: 0.6902406036575304\n",
      "Iteration: 22\n",
      "RMSE: 0.87847755915396\n",
      "MAE: 0.6896474935982955\n",
      "Iteration: 23\n",
      "RMSE: 0.877937638621396\n",
      "MAE: 0.689111045954278\n",
      "Iteration: 24\n",
      "RMSE: 0.8774552416397046\n",
      "MAE: 0.6886273214027968\n",
      "Iteration: 25\n",
      "RMSE: 0.8770246308041204\n",
      "MAE: 0.6881906471686295\n",
      "Iteration: 26\n",
      "RMSE: 0.876640735144817\n",
      "MAE: 0.6877911641447425\n",
      "Iteration: 27\n",
      "RMSE: 0.8762990205493308\n",
      "MAE: 0.6874293373006203\n",
      "Iteration: 28\n",
      "RMSE: 0.8759954022740725\n",
      "MAE: 0.6871063334303453\n",
      "Iteration: 29\n",
      "RMSE: 0.8757261835022744\n",
      "MAE: 0.6868143959730578\n",
      "Iteration: 30\n",
      "RMSE: 0.8754880101305502\n",
      "MAE: 0.6865536901290467\n",
      "Iteration: 31\n",
      "RMSE: 0.875277835806508\n",
      "MAE: 0.6863153256069734\n",
      "Iteration: 32\n",
      "RMSE: 0.875092893564314\n",
      "MAE: 0.6860984803698071\n",
      "Iteration: 33\n",
      "RMSE: 0.8749306718062599\n",
      "MAE: 0.6859032382367568\n",
      "Iteration: 34\n",
      "RMSE: 0.8747888932299114\n",
      "MAE: 0.6857286673405749\n",
      "Iteration: 35\n",
      "RMSE: 0.8746654958263786\n",
      "MAE: 0.6855727385479872\n",
      "Iteration: 36\n",
      "RMSE: 0.8745586154063729\n",
      "MAE: 0.6854340224425044\n",
      "Iteration: 37\n",
      "RMSE: 0.8744665693224981\n",
      "MAE: 0.6853096851646334\n",
      "Iteration: 38\n",
      "RMSE: 0.8743878411926792\n",
      "MAE: 0.6851989918610979\n",
      "Iteration: 39\n",
      "RMSE: 0.8743210665169573\n",
      "MAE: 0.6850996240602127\n",
      "Iteration: 40\n",
      "RMSE: 0.8742650191342607\n",
      "MAE: 0.6850097383043519\n",
      "Iteration: 41\n",
      "RMSE: 0.8742185984976016\n",
      "MAE: 0.6849278987668285\n",
      "Iteration: 42\n",
      "RMSE: 0.8741808177624789\n",
      "MAE: 0.6848548860577328\n",
      "Iteration: 43\n",
      "RMSE: 0.874150792689041\n",
      "MAE: 0.6847910173692653\n",
      "Iteration: 44\n",
      "RMSE: 0.8741277313574062\n",
      "MAE: 0.6847346888945033\n",
      "Iteration: 45\n",
      "RMSE: 0.8741109246901568\n",
      "MAE: 0.6846868048250621\n",
      "Iteration: 46\n",
      "RMSE: 0.8740997377684441\n",
      "MAE: 0.684643804569224\n",
      "Iteration: 47\n",
      "RMSE: 0.8740936019198468\n",
      "MAE: 0.6846053328694404\n",
      "Iteration: 48\n",
      "RMSE: 0.8740920075481831\n",
      "MAE: 0.6845716487380248\n",
      "Iteration: 49\n",
      "RMSE: 0.8740944976685461\n",
      "MAE: 0.6845416530392977\n",
      "Iteration: 50\n",
      "RMSE: 0.8741006621053573\n",
      "MAE: 0.6845147084863384\n",
      "Iteration: 51\n",
      "RMSE: 0.8741101323073298\n",
      "MAE: 0.6844903644238427\n",
      "Iteration: 52\n",
      "RMSE: 0.874122576730946\n",
      "MAE: 0.6844676823963688\n",
      "Iteration: 53\n",
      "RMSE: 0.8741376967432188\n",
      "MAE: 0.6844484163697826\n",
      "Iteration: 54\n",
      "RMSE: 0.8741552229949626\n",
      "MAE: 0.6844320282408934\n",
      "Iteration: 55\n",
      "RMSE: 0.8741749122172836\n",
      "MAE: 0.6844182095038981\n",
      "Iteration: 56\n",
      "RMSE: 0.8741965443962889\n",
      "MAE: 0.6844077300478321\n",
      "Iteration: 57\n",
      "RMSE: 0.874219920283864\n",
      "MAE: 0.6843996450189178\n",
      "Iteration: 58\n",
      "RMSE: 0.8742448592055825\n",
      "MAE: 0.6843947444109622\n",
      "Iteration: 59\n",
      "RMSE: 0.874271197130203\n",
      "MAE: 0.6843909141476731\n",
      "Iteration: 60\n",
      "RMSE: 0.8742987849686412\n",
      "MAE: 0.6843881078116969\n",
      "Iteration: 61\n",
      "RMSE: 0.8743274870736664\n",
      "MAE: 0.6843865065261205\n",
      "Iteration: 62\n",
      "RMSE: 0.8743571799147829\n",
      "MAE: 0.6843869507263765\n",
      "Iteration: 63\n",
      "RMSE: 0.8743877509057659\n",
      "MAE: 0.6843894406060336\n",
      "Iteration: 64\n",
      "RMSE: 0.8744190973650856\n",
      "MAE: 0.6843925343123719\n",
      "Iteration: 65\n",
      "RMSE: 0.8744511255919749\n",
      "MAE: 0.6843959219541131\n",
      "Iteration: 66\n",
      "RMSE: 0.8744837500431528\n",
      "MAE: 0.6844012237848344\n",
      "Iteration: 67\n",
      "RMSE: 0.8745168925972275\n",
      "MAE: 0.6844071690767711\n",
      "Iteration: 68\n",
      "RMSE: 0.8745504818955817\n",
      "MAE: 0.684413721269602\n",
      "Iteration: 69\n",
      "RMSE: 0.8745844527500868\n",
      "MAE: 0.6844207351015539\n",
      "Iteration: 70\n",
      "RMSE: 0.8746187456093604\n",
      "MAE: 0.684428482729199\n",
      "Iteration: 71\n",
      "RMSE: 0.8746533060764398\n",
      "MAE: 0.684436093906652\n",
      "Iteration: 72\n",
      "RMSE: 0.8746880844717619\n",
      "MAE: 0.6844435852576152\n",
      "Iteration: 73\n",
      "RMSE: 0.8747230354362072\n",
      "MAE: 0.6844517327354467\n",
      "Iteration: 74\n",
      "RMSE: 0.874758117569701\n",
      "MAE: 0.6844609785418375\n",
      "Iteration: 0\n",
      "RMSE: 0.9402702563205729\n",
      "MAE: 0.7455057935004528\n",
      "Iteration: 1\n",
      "RMSE: 0.9327809373121827\n",
      "MAE: 0.7389169184919144\n",
      "Iteration: 2\n",
      "RMSE: 0.9299455618901351\n",
      "MAE: 0.7364596276383387\n",
      "Iteration: 3\n",
      "RMSE: 0.9265180831596959\n",
      "MAE: 0.7334877477851799\n",
      "Iteration: 4\n",
      "RMSE: 0.9210919968874939\n",
      "MAE: 0.7286989413793457\n",
      "Iteration: 5\n",
      "RMSE: 0.9150399669669751\n",
      "MAE: 0.7232515415084574\n",
      "Iteration: 6\n",
      "RMSE: 0.9098213565209639\n",
      "MAE: 0.7185505399084291\n",
      "Iteration: 7\n",
      "RMSE: 0.9053771411020091\n",
      "MAE: 0.7146268598168188\n",
      "Iteration: 8\n",
      "RMSE: 0.9014031341232148\n",
      "MAE: 0.7111614903332591\n",
      "Iteration: 9\n",
      "RMSE: 0.8977612850114864\n",
      "MAE: 0.7079685510061553\n",
      "Iteration: 10\n",
      "RMSE: 0.894431602255413\n",
      "MAE: 0.7050417927957738\n",
      "Iteration: 11\n",
      "RMSE: 0.8914309455664603\n",
      "MAE: 0.7023960801601\n",
      "Iteration: 12\n",
      "RMSE: 0.8887639562177091\n",
      "MAE: 0.7000049322194322\n",
      "Iteration: 13\n",
      "RMSE: 0.8864147568572213\n",
      "MAE: 0.6978814894173819\n",
      "Iteration: 14\n",
      "RMSE: 0.8843561146391853\n",
      "MAE: 0.696028300488727\n",
      "Iteration: 15\n",
      "RMSE: 0.8825576117654599\n",
      "MAE: 0.6943917585298471\n",
      "Iteration: 16\n",
      "RMSE: 0.8809895628132964\n",
      "MAE: 0.6929676068632642\n",
      "Iteration: 17\n",
      "RMSE: 0.8796244654085841\n",
      "MAE: 0.6917355934422215\n",
      "Iteration: 18\n",
      "RMSE: 0.8784374283814421\n",
      "MAE: 0.6906499709189464\n",
      "Iteration: 19\n",
      "RMSE: 0.8774061965574558\n",
      "MAE: 0.6896909557652403\n",
      "Iteration: 20\n",
      "RMSE: 0.8765110090558434\n",
      "MAE: 0.6888511905979874\n",
      "Iteration: 21\n",
      "RMSE: 0.8757343957135585\n",
      "MAE: 0.6881161996338179\n",
      "Iteration: 22\n",
      "RMSE: 0.8750609630650285\n",
      "MAE: 0.6874763223644921\n",
      "Iteration: 23\n",
      "RMSE: 0.8744771916701479\n",
      "MAE: 0.6869231124172224\n",
      "Iteration: 24\n",
      "RMSE: 0.8739712498186168\n",
      "MAE: 0.6864387837839881\n",
      "Iteration: 25\n",
      "RMSE: 0.8735328215744927\n",
      "MAE: 0.6860094204297947\n",
      "Iteration: 26\n",
      "RMSE: 0.8731529460242704\n",
      "MAE: 0.6856340054424115\n",
      "Iteration: 27\n",
      "RMSE: 0.872823865936331\n",
      "MAE: 0.685305585351173\n",
      "Iteration: 28\n",
      "RMSE: 0.872538885624642\n",
      "MAE: 0.6850156348511848\n",
      "Iteration: 29\n",
      "RMSE: 0.8722922387507618\n",
      "MAE: 0.6847564364441601\n",
      "Iteration: 30\n",
      "RMSE: 0.87207896702014\n",
      "MAE: 0.6845258483899731\n",
      "Iteration: 31\n",
      "RMSE: 0.8718948104746581\n",
      "MAE: 0.6843200768736977\n",
      "Iteration: 32\n",
      "RMSE: 0.8717361096272481\n",
      "MAE: 0.6841356042933109\n",
      "Iteration: 33\n",
      "RMSE: 0.8715997192192569\n",
      "MAE: 0.6839740450526692\n",
      "Iteration: 34\n",
      "RMSE: 0.8714829330066074\n",
      "MAE: 0.6838323320216337\n",
      "Iteration: 35\n",
      "RMSE: 0.8713834187280602\n",
      "MAE: 0.6837021874403738\n",
      "Iteration: 36\n",
      "RMSE: 0.8712991622707286\n",
      "MAE: 0.6835847795166767\n",
      "Iteration: 37\n",
      "RMSE: 0.8712284200009213\n",
      "MAE: 0.6834789469467548\n",
      "Iteration: 38\n",
      "RMSE: 0.8711696782458226\n",
      "MAE: 0.6833851790949061\n",
      "Iteration: 39\n",
      "RMSE: 0.8711216189700914\n",
      "MAE: 0.6833026023570674\n",
      "Iteration: 40\n",
      "RMSE: 0.8710830907731445\n",
      "MAE: 0.6832282896989794\n",
      "Iteration: 41\n",
      "RMSE: 0.8710530844248131\n",
      "MAE: 0.6831607194616045\n",
      "Iteration: 42\n",
      "RMSE: 0.8710307122507261\n",
      "MAE: 0.6830988161230734\n",
      "Iteration: 43\n",
      "RMSE: 0.8710151907688675\n",
      "MAE: 0.683042807530462\n",
      "Iteration: 44\n",
      "RMSE: 0.871005826062261\n",
      "MAE: 0.6829909469028798\n",
      "Iteration: 45\n",
      "RMSE: 0.8710020014481673\n",
      "MAE: 0.6829445604273541\n",
      "Iteration: 46\n",
      "RMSE: 0.8710031670710376\n",
      "MAE: 0.6829057707877408\n",
      "Iteration: 47\n",
      "RMSE: 0.8710088311048578\n",
      "MAE: 0.6828716565775799\n",
      "Iteration: 48\n",
      "RMSE: 0.8710185523009419\n",
      "MAE: 0.6828423577096726\n",
      "Iteration: 49\n",
      "RMSE: 0.871031933660342\n",
      "MAE: 0.6828168485609207\n",
      "Iteration: 50\n",
      "RMSE: 0.8710486170466339\n",
      "MAE: 0.6827942348754561\n",
      "Iteration: 51\n",
      "RMSE: 0.8710682785856698\n",
      "MAE: 0.6827742443053408\n",
      "Iteration: 52\n",
      "RMSE: 0.8710906247247339\n",
      "MAE: 0.6827584644981093\n",
      "Iteration: 53\n",
      "RMSE: 0.8711153888451176\n",
      "MAE: 0.6827455331391328\n",
      "Iteration: 54\n",
      "RMSE: 0.8711423283400622\n",
      "MAE: 0.6827344880561912\n",
      "Iteration: 55\n",
      "RMSE: 0.8711712220848903\n",
      "MAE: 0.6827248981719968\n",
      "Iteration: 56\n",
      "RMSE: 0.8712018682384687\n",
      "MAE: 0.6827175478102365\n",
      "Iteration: 57\n",
      "RMSE: 0.8712340823253125\n",
      "MAE: 0.682711836381811\n",
      "Iteration: 58\n",
      "RMSE: 0.8712676955560597\n",
      "MAE: 0.682706855623944\n",
      "Iteration: 59\n",
      "RMSE: 0.8713025533509916\n",
      "MAE: 0.6827039066166053\n",
      "Iteration: 60\n",
      "RMSE: 0.8713385140370246\n",
      "MAE: 0.6827023852577825\n",
      "Iteration: 61\n",
      "RMSE: 0.8713754476933531\n",
      "MAE: 0.6827020587033188\n",
      "Iteration: 62\n",
      "RMSE: 0.8714132351248761\n",
      "MAE: 0.6827035815115878\n",
      "Iteration: 63\n",
      "RMSE: 0.8714517669458091\n",
      "MAE: 0.6827052421201701\n",
      "Iteration: 64\n",
      "RMSE: 0.8714909427586193\n",
      "MAE: 0.6827084700732948\n",
      "Iteration: 65\n",
      "RMSE: 0.871530670415695\n",
      "MAE: 0.682711972413087\n",
      "Iteration: 66\n",
      "RMSE: 0.8715708653530667\n",
      "MAE: 0.6827155566964217\n",
      "Iteration: 67\n",
      "RMSE: 0.8716114499870977\n",
      "MAE: 0.6827207751446797\n",
      "Iteration: 68\n",
      "RMSE: 0.8716523531664034\n",
      "MAE: 0.6827266606519731\n",
      "Iteration: 69\n",
      "RMSE: 0.8716935096723932\n",
      "MAE: 0.6827334891652679\n",
      "Iteration: 70\n",
      "RMSE: 0.871734859762788\n",
      "MAE: 0.6827405382629875\n",
      "Iteration: 71\n",
      "RMSE: 0.8717763487532741\n",
      "MAE: 0.6827480937965646\n",
      "Iteration: 72\n",
      "RMSE: 0.871817926633148\n",
      "MAE: 0.6827556993885388\n",
      "Iteration: 73\n",
      "RMSE: 0.8718595477113783\n",
      "MAE: 0.6827637071043534\n",
      "Iteration: 74\n",
      "RMSE: 0.8719011702900236\n",
      "MAE: 0.6827723554788754\n",
      "Iteration: 0\n",
      "RMSE: 0.9398462234738489\n",
      "MAE: 0.7456030907248011\n",
      "Iteration: 1\n",
      "RMSE: 0.9323003502659386\n",
      "MAE: 0.7390423250466375\n",
      "Iteration: 2\n",
      "RMSE: 0.929837412374681\n",
      "MAE: 0.7368490461712629\n",
      "Iteration: 3\n",
      "RMSE: 0.9274511857477694\n",
      "MAE: 0.7347610463078235\n",
      "Iteration: 4\n",
      "RMSE: 0.9232375667999989\n",
      "MAE: 0.7311312320909106\n",
      "Iteration: 5\n",
      "RMSE: 0.9166852056978712\n",
      "MAE: 0.7254543123353153\n",
      "Iteration: 6\n",
      "RMSE: 0.9095974643298168\n",
      "MAE: 0.7192623462411167\n",
      "Iteration: 7\n",
      "RMSE: 0.9034938182656748\n",
      "MAE: 0.7138316602399942\n",
      "Iteration: 8\n",
      "RMSE: 0.8986163538435005\n",
      "MAE: 0.7094114560086466\n",
      "Iteration: 9\n",
      "RMSE: 0.8947953124181351\n",
      "MAE: 0.7058874394321506\n",
      "Iteration: 10\n",
      "RMSE: 0.8917944333878591\n",
      "MAE: 0.7030432372501104\n",
      "Iteration: 11\n",
      "RMSE: 0.8893988946772583\n",
      "MAE: 0.7007253898881006\n",
      "Iteration: 12\n",
      "RMSE: 0.8874395755202147\n",
      "MAE: 0.6987975256987631\n",
      "Iteration: 13\n",
      "RMSE: 0.8857928583557069\n",
      "MAE: 0.6971532514222177\n",
      "Iteration: 14\n",
      "RMSE: 0.8843725706954491\n",
      "MAE: 0.695722539069177\n",
      "Iteration: 15\n",
      "RMSE: 0.8831206175494916\n",
      "MAE: 0.6944690630941047\n",
      "Iteration: 16\n",
      "RMSE: 0.8819986895411802\n",
      "MAE: 0.6933518568129569\n",
      "Iteration: 17\n",
      "RMSE: 0.8809817850013553\n",
      "MAE: 0.6923423270504249\n",
      "Iteration: 18\n",
      "RMSE: 0.8800535271602712\n",
      "MAE: 0.6914280685721583\n",
      "Iteration: 19\n",
      "RMSE: 0.8792029539103104\n",
      "MAE: 0.6905927131685613\n",
      "Iteration: 20\n",
      "RMSE: 0.8784224006847081\n",
      "MAE: 0.6898272875692599\n",
      "Iteration: 21\n",
      "RMSE: 0.877706148255129\n",
      "MAE: 0.6891226283700995\n",
      "Iteration: 22\n",
      "RMSE: 0.8770495865807206\n",
      "MAE: 0.6884712790475509\n",
      "Iteration: 23\n",
      "RMSE: 0.8764487178939266\n",
      "MAE: 0.6878663955258614\n",
      "Iteration: 24\n",
      "RMSE: 0.8758998773712671\n",
      "MAE: 0.687306373214373\n",
      "Iteration: 25\n",
      "RMSE: 0.8753995891684009\n",
      "MAE: 0.6867943401556365\n",
      "Iteration: 26\n",
      "RMSE: 0.8749445031330572\n",
      "MAE: 0.6863236686095795\n",
      "Iteration: 27\n",
      "RMSE: 0.8745313766099148\n",
      "MAE: 0.6858902266585577\n",
      "Iteration: 28\n",
      "RMSE: 0.8741570789090609\n",
      "MAE: 0.6854908732175904\n",
      "Iteration: 29\n",
      "RMSE: 0.8738186049325019\n",
      "MAE: 0.6851263057487817\n",
      "Iteration: 30\n",
      "RMSE: 0.87351309034597\n",
      "MAE: 0.6847920248838342\n",
      "Iteration: 31\n",
      "RMSE: 0.8732378244321528\n",
      "MAE: 0.6844873090432326\n",
      "Iteration: 32\n",
      "RMSE: 0.8729902590319177\n",
      "MAE: 0.6842099734095685\n",
      "Iteration: 33\n",
      "RMSE: 0.8727680132672792\n",
      "MAE: 0.683954475971658\n",
      "Iteration: 34\n",
      "RMSE: 0.8725688744013883\n",
      "MAE: 0.6837213496959765\n",
      "Iteration: 35\n",
      "RMSE: 0.8723907954747098\n",
      "MAE: 0.6835091100329176\n",
      "Iteration: 36\n",
      "RMSE: 0.8722318904268593\n",
      "MAE: 0.6833149763070758\n",
      "Iteration: 37\n",
      "RMSE: 0.8720904273736959\n",
      "MAE: 0.683141060384848\n",
      "Iteration: 38\n",
      "RMSE: 0.871964820620811\n",
      "MAE: 0.6829804354438481\n",
      "Iteration: 39\n",
      "RMSE: 0.8718536218920382\n",
      "MAE: 0.6828336813221147\n",
      "Iteration: 40\n",
      "RMSE: 0.8717555111527284\n",
      "MAE: 0.6827008204895215\n",
      "Iteration: 41\n",
      "RMSE: 0.8716692873201805\n",
      "MAE: 0.6825799980913579\n",
      "Iteration: 42\n",
      "RMSE: 0.8715938590802289\n",
      "MAE: 0.682470402298039\n",
      "Iteration: 43\n",
      "RMSE: 0.8715282359692902\n",
      "MAE: 0.6823716867871995\n",
      "Iteration: 44\n",
      "RMSE: 0.8714715198336233\n",
      "MAE: 0.6822813152295747\n",
      "Iteration: 45\n",
      "RMSE: 0.8714228967402856\n",
      "MAE: 0.6821983143275376\n",
      "Iteration: 46\n",
      "RMSE: 0.8713816293853652\n",
      "MAE: 0.6821222330667004\n",
      "Iteration: 47\n",
      "RMSE: 0.8713470500229287\n",
      "MAE: 0.6820518277500282\n",
      "Iteration: 48\n",
      "RMSE: 0.8713185539213151\n",
      "MAE: 0.6819881473510394\n",
      "Iteration: 49\n",
      "RMSE: 0.8712955933408512\n",
      "MAE: 0.6819312342498739\n",
      "Iteration: 50\n",
      "RMSE: 0.8712776720178511\n",
      "MAE: 0.6818801637741886\n",
      "Iteration: 51\n",
      "RMSE: 0.8712643401332105\n",
      "MAE: 0.6818344813714768\n",
      "Iteration: 52\n",
      "RMSE: 0.87125518973944\n",
      "MAE: 0.681790777761413\n",
      "Iteration: 53\n",
      "RMSE: 0.871249850617203\n",
      "MAE: 0.6817507433817012\n",
      "Iteration: 54\n",
      "RMSE: 0.8712479865309277\n",
      "MAE: 0.6817142536728322\n",
      "Iteration: 55\n",
      "RMSE: 0.8712492918526069\n",
      "MAE: 0.6816811764658411\n",
      "Iteration: 56\n",
      "RMSE: 0.8712534885232159\n",
      "MAE: 0.6816508520700325\n",
      "Iteration: 57\n",
      "RMSE: 0.8712603233220932\n",
      "MAE: 0.6816230362694646\n",
      "Iteration: 58\n",
      "RMSE: 0.8712695654159511\n",
      "MAE: 0.6815996385494815\n",
      "Iteration: 59\n",
      "RMSE: 0.8712810041608167\n",
      "MAE: 0.6815787833960246\n",
      "Iteration: 60\n",
      "RMSE: 0.8712944471319939\n",
      "MAE: 0.6815608092607\n",
      "Iteration: 61\n",
      "RMSE: 0.8713097183590411\n",
      "MAE: 0.6815448630340408\n",
      "Iteration: 62\n",
      "RMSE: 0.8713266567446575\n",
      "MAE: 0.6815311405871215\n",
      "Iteration: 63\n",
      "RMSE: 0.8713451146482711\n",
      "MAE: 0.6815185920343547\n",
      "Iteration: 64\n",
      "RMSE: 0.8713649566169199\n",
      "MAE: 0.6815087569891165\n",
      "Iteration: 65\n",
      "RMSE: 0.871386058247736\n",
      "MAE: 0.6815003125999852\n",
      "Iteration: 66\n",
      "RMSE: 0.8714083051679451\n",
      "MAE: 0.6814937102148563\n",
      "Iteration: 67\n",
      "RMSE: 0.8714315921197612\n",
      "MAE: 0.6814893045748159\n",
      "Iteration: 68\n",
      "RMSE: 0.8714558221389045\n",
      "MAE: 0.6814861958677353\n",
      "Iteration: 69\n",
      "RMSE: 0.8714809058166935\n",
      "MAE: 0.6814842924820463\n",
      "Iteration: 70\n",
      "RMSE: 0.8715067606367515\n",
      "MAE: 0.6814829625988476\n",
      "Iteration: 71\n",
      "RMSE: 0.8715333103783538\n",
      "MAE: 0.6814833868075433\n",
      "Iteration: 72\n",
      "RMSE: 0.8715604845793168\n",
      "MAE: 0.681485213420169\n",
      "Iteration: 73\n",
      "RMSE: 0.8715882180521043\n",
      "MAE: 0.6814877384763925\n",
      "Iteration: 74\n",
      "RMSE: 0.8716164504475197\n",
      "MAE: 0.681490900904747\n"
     ]
    }
   ],
   "source": [
    "cv = K_fold(n_splits=5, seed=42, data=ratings, shuffle=True)\n",
    "\n",
    "num_factors = 10\n",
    "num_iter=75\n",
    "regularization = 0.05\n",
    "learning_rate=0.005\n",
    "\n",
    "# Initialize variables to track the best results\n",
    "lowest_rmse = float('inf')\n",
    "best_U = None\n",
    "best_M = None\n",
    "\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "\n",
    "for split, (train_index, test_index) in enumerate(cv):\n",
    "    train_data, test_data = ratings.iloc[train_index], ratings.iloc[test_index]\n",
    "\n",
    "    # Initialize user and movie matrices U and M\n",
    "    U = np.random.random((6040,num_factors))\n",
    "    M = np.random.random((num_factors,3952))\n",
    "    X = train_data.to_numpy()\n",
    "\n",
    "    rmse_split = []\n",
    "    mae_split = []\n",
    "\n",
    "    prev_rmse = float('inf')\n",
    "    consecutive_iterations = 0\n",
    "\n",
    "    for iter in range(num_iter):\n",
    "        print('Iteration:', iter)\n",
    "        for (u,m,r,_) in X:\n",
    "            i,j = u-1,m-1\n",
    "\n",
    "            # Calculate the error eij\n",
    "            eij = r - np.dot(U[i,:],M[:,j])\n",
    "\n",
    "            # Update user and movie matrices using gradient descent\n",
    "            for k in range(num_factors):\n",
    "                U[i][k] = U[i][k] + learning_rate * (2 * eij * M[k][j] - regularization * U[i][k])\n",
    "                M[k][j] = M[k][j] + learning_rate * (2 * eij * U[i][k] - regularization * M[k][j])\n",
    "\n",
    "        # Calculate RMSE and store it\n",
    "        rmse_val, mae_val = calculate_rmse(np.dot(U,M),test_data.to_numpy())\n",
    "        rmse_split.append(rmse_val)\n",
    "        mae_split.append(mae_val)\n",
    "        print(\"RMSE:\", rmse_val)\n",
    "        print(\"MAE:\", mae_val)\n",
    "\n",
    "        # Check for RMSE convergence\n",
    "        if rmse_val >= prev_rmse:\n",
    "            consecutive_iterations += 1\n",
    "            if consecutive_iterations == 2:\n",
    "                break # Terminate the loop if RMSE doesn't decrease for two consecutive iterations\n",
    "        else:\n",
    "            consecutive_iterations = 0\n",
    "        prev_rmse = rmse_val\n",
    "\n",
    "    rmse_list.append(rmse_split[-1])\n",
    "    mae_list.append(mae_split[-1])\n",
    "\n",
    "    # Update the best results if a lower RMSE is found\n",
    "    if min(rmse_split) < lowest_rmse:\n",
    "        lowest_rmse = min(rmse_split)\n",
    "        best_U = U\n",
    "        best_M = M"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE list: [0.6832686411084417, 0.6844264088019703, 0.6844609785418375, 0.6827723554788754, 0.681490900904747]\n",
      "Average MAE: 0.6832838569671744\n",
      "RMSE list: [0.8730343927259353, 0.8752211592905211, 0.874758117569701, 0.8719011702900236, 0.8716164504475197]\n",
      "Average RMSE: 0.8733062580647403\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"MAE list:\", mae_list)\n",
    "print(\"Average MAE:\", sum(mae_list) / len(mae_list))\n",
    "print(\"RMSE list:\", rmse_list)\n",
    "print(\"Average RMSE:\", sum(rmse_list) / len(rmse_list))\n",
    "\n",
    "Save the U and M matrices weights from the best performed iteration\n",
    "print(\"Lowest rmse:\", lowest_rmse)\n",
    "os.makedirs(\"feature_matrices\", exist_ok=True)\n",
    "np.savetxt(os.path.join(\"feature_matrices\", \"movies.csv\"), best_M.transpose(), delimiter=',')\n",
    "np.savetxt(os.path.join(\"feature_matrices\", \"users.csv\"), best_U, delimiter=',')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
